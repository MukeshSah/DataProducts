Then, we read the generated csv file. If the data already exists in the working environment, we do not need to load it again. Otherwise, we read the csv file.
```{r}
if (!"StormData" %in% ls()) {
stormData <- read.csv("/home/tensor/Documents/Tutorials/R/FiftH/RepData_PeerAssessment2-master/StormData.csv", sep = ",")
}
dim(stormData)
head(stormData, n = 2)
```
There are 902297 rows and 37 columns in total.
The events in the database start in the year 1950 and end in November 2011. In the earlier years of the database there are generally fewer events recorded, most likely due to a lack of good records. More recent years should be considered more complete.
```{r}
if (dim(stormData)[2] == 37) {
stormData$year <- as.numeric(format(as.Date(stormData$BGN_DATE, format = "%m/%d/%Y %H:%M:%S"), "%Y"))
}
hist(stormData$year, breaks = 30)
```
Based on the above histogram, we see that the number of events tracked starts to significantly increase around 1995. So, we use the subset of the data from 1990 to 2011 to get most out of good records.
```{r}
storm <- stormData[stormData$year >= 1995, ]
dim(storm)
```
Now, there are 681500 rows and 38 columns in total.
### Impact on Public Health
In this section, we check the number of **fatalities** and **injuries** that are caused by the severe weather events. We would like to get the first 15 most severe types of weather events.
```{r}
sortHelper <- function(fieldName, top = 15, dataset = stormData) {
index <- which(colnames(dataset) == fieldName)
field <- aggregate(dataset[, index], by = list(dataset$EVTYPE), FUN = "sum")
names(field) <- c("EVTYPE", fieldName)
field <- arrange(field, field[, 2], decreasing = T)
field <- head(field, n = top)
field <- within(field, EVTYPE <- factor(x = EVTYPE, levels = field$EVTYPE))
return(field)
}
fatalities <- sortHelper("FATALITIES", dataset = storm)
injuries <- sortHelper("INJURIES", dataset = storm)
```
#### Impact on Economy
We will convert the **property damage** and **crop damage** data into comparable numerical forms according to the meaning of units described in the code book ([Storm Events](http://ire.org/nicar/database-library/databases/storm-events/)). Both `PROPDMGEXP` and `CROPDMGEXP` columns record a multiplier for each observation where we have Hundred (H), Thousand (K), Million (M) and Billion (B).
```{r}
convertHelper <- function(dataset = storm, fieldName, newFieldName) {
totalLen <- dim(dataset)[2]
index <- which(colnames(dataset) == fieldName)
dataset[, index] <- as.character(dataset[, index])
logic <- !is.na(toupper(dataset[, index]))
dataset[logic & toupper(dataset[, index]) == "B", index] <- "9"
dataset[logic & toupper(dataset[, index]) == "M", index] <- "6"
dataset[logic & toupper(dataset[, index]) == "K", index] <- "3"
dataset[logic & toupper(dataset[, index]) == "H", index] <- "2"
dataset[logic & toupper(dataset[, index]) == "", index] <- "0"
dataset[, index] <- as.numeric(dataset[, index])
dataset[is.na(dataset[, index]), index] <- 0
dataset <- cbind(dataset, dataset[, index - 1] * 10^dataset[, index])
names(dataset)[totalLen + 1] <- newFieldName
return(dataset)
}
storm <- convertHelper(storm, "PROPDMGEXP", "propertyDamage")
storm <- convertHelper(storm, "CROPDMGEXP", "cropDamage")
names(storm)
options(scipen=999)
property <- sortHelper("propertyDamage", dataset = storm)
crop <- sortHelper("cropDamage", dataset = storm)
```
### Results
As for the impact on public health, we have got two sorted lists of severe weather events below by the number of people badly affected.
```{r}
fatalities
injuries
```
And the following is a pair of graphs of total fatalities and total injuries affected by these severe weather events.
```{r}
fatalitiesPlot <- qplot(EVTYPE, data = fatalities, weight = FATALITIES, geom = "bar", binwidth = 1) +
scale_y_continuous("Number of Fatalities") +
theme(axis.text.x = element_text(angle = 45,
hjust = 1)) + xlab("Severe Weather Type") +
ggtitle("Total Fatalities by Severe Weather\n Events in the U.S.\n from 1995 - 2011")
injuriesPlot <- qplot(EVTYPE, data = injuries, weight = INJURIES, geom = "bar", binwidth = 1) +
scale_y_continuous("Number of Injuries") +
theme(axis.text.x = element_text(angle = 45,
hjust = 1)) + xlab("Severe Weather Type") +
ggtitle("Total Injuries by Severe Weather\n Events in the U.S.\n from 1995 - 2011")
grid.arrange(fatalitiesPlot, injuriesPlot, ncol = 2)
```
Based on the above histograms, we find that **excessive heat** and **tornado** cause most fatalities; **tornato** causes most injuries in the United States from 1995 to 2011.
As for the impact on economy, we have got two sorted lists below by the amount of money cost by damages.
```{r}
property
crop
```
And the following is a pair of graphs of total property damage and total crop damage affected by these severe weather events.
```{r}
propertyPlot <- qplot(EVTYPE, data = property, weight = propertyDamage, geom = "bar", binwidth = 1) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_y_continuous("Property Damage in US dollars")+
xlab("Severe Weather Type") + ggtitle("Total Property Damage by\n Severe Weather Events in\n the U.S. from 1995 - 2011")
cropPlot<- qplot(EVTYPE, data = crop, weight = cropDamage, geom = "bar", binwidth = 1) +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) + scale_y_continuous("Crop Damage in US dollars") +
xlab("Severe Weather Type") + ggtitle("Total Crop Damage by \nSevere Weather Events in\n the U.S. from 1995 - 2011")
grid.arrange(propertyPlot, cropPlot, ncol = 2)
```
Based on the above histograms, we find that **flood** and **hurricane/typhoon** cause most property damage; **drought** and **flood** causes most crop damage in the United States from 1995 to 2011.
### Conclusion
From these data, we found that **excessive heat** and **tornado** are most harmful with respect to population health, while **flood**, **drought**, and **hurricane/typhoon** have the greatest economic consequences.
install.packages("swirl")
packageVersion("swirl")
library(swirl)
install_from_swirl("Statistical Inference")
install_from_swirl("Statistical Inference")
library("swirl")
install_from_swirl("Statistical Inference")
swirl()
library("swirl")
swirl()
11/12
deck
deck.length()
deck.size()
len(deck)
length(deck)
4/52
0
12/52
2/51
1.6*0.8
(1.6*0.8)/2
0.64
mypdf
?integrate
integrate(mypdf,0,1.6)
1
sqrt(2)
0.000997
0.984015
0.014985
0.06238
(0.997*0.001)/(0.997*0.001+0.015*0.999)
1/6
1/36
3.5
expect_dice
dice_high
expect_dice(dice_hight)
expect_dice(dice_high)
expect_dice(dice_low)
0.5(edh+edl)
0.5*(edh+edl)
swirl()
integrate(myfunc,0,2)
spop
spop.mean()
mean(spop)
allsam
apply(allsam,1,mean)
mean(smeans)
exit
library("swirl")
swirl()
dice_sqr
ex2_fair<-dice_fair*dice_sqr
ex2_fair<-sum(dice_fair*dice_sqr)
ex2_fair-3.5
ex2_fair-3.5^2
sum(dice_high*dice_sqr)-edh^2
sd(apply(matrix(rnorm(10000),1000),1,mean))
1/sqrt(10)
1/sqrt(120)
sd(apply(matrix(runif(10000),1000),1,mean))
2/sqrt(10)
sd(apply(matrix(rpois(10000,4),1000),1,mean))
1/(2*sqrt(10))
sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
choose(5,3)*((0.8)^3)*((1-0.8)^(5-3))
choose(5,3)*((0.8)^3)*((0.2)^(5-3))
choose(5,3)*((0.8)^3:5)*((0.2)^(5-3:5))
choose(5,3)*((0.8)^3)*((0.2)^(5-3))+choose(5,4)*((0.8)^4)*((0.2)^(5-4))+choose(5,5)*((0.8)^5)*((0.2)^(5-5))
pbinom(5,2,0.8,lower.tail=FALSE)
pbinom(2,size=5,prob=.8,lower.tail=FALSE)
qnorm(10)
qnorm(0.1)
0
qnorm(3,2)
qnorm(0.975,mean=3,sd=2)
3+1.96*2
pnorm(1200,mean=1020,sd=50)
1-pnorm(1200,mean=1020,sd=50)
1-pnorm(1200,mean=1020,sd=50,lower.tail=FALSE)
pnorm(1200,mean=1020,sd=50,lower.tail=FALSE)
pnorm()
pnorm(1200)
pnorm((1200-1020)/50,lower.tail=FALSE)
pnorm(0.75,mean=1020,sd=50)
qnorm(0.75,mean=1020,sd=50)
pnorm(qnorm(0.53))
0.53
ppois(3,mean=2.5*4)
ppois(3,mean=10)
ppois(3,10)
ppois(3,2.5*4)
pbinom(5,1000,0.01)
ppois(5,1000*0.01)
coinPlot(n=10)
coinPlot(n=10000)
qnorm(95)
qnorm(0.95)
p
p+/-qnorm
p'+c(-1,1)
;
as
ad
''
q()
swirl()
library("swirl")
swirl()
p''
p'
;
''
a
qnorm(0.875)
0.6+c(-1,1)*qnorm(0.975)*sqrt(0.6*.4/100)
x$conf.int(60,100)
binom.test$conf.int(60,100)
binom.test(conf.int(60,100))
conf.int(60,100)
binom.test(60,100)$conf.int
mywald(.2)
ACCompar(20)
lamb=5/94.32
lamb<-5/94.32
lamb+c(-1,1)*qnorm(.975)*sqrt(lamb/94.32)
binom.test(5,94.32)$conf.int
poisson.test(5,94.32)$conf.int
poisson.test(5,94.32)$conf
pbinom(70,mean=80,sd=10)
pbinom(70,80,10)
qbinom(70,80,10)
qbinom(70,mean=8,sd=0,10)
qnom(70,mean=8,sd=0,10)
qnorm(70,mean=8,sd=0,10)
pnorm(70,mean=8,sd=0,10)
pnorm(70,mean=8,sd=0.10)
qnorm(70,mean=8,sd=0.10)
qnorm(0.7,mean=8,sd=0.10)
pnorm(0.7,mean=8,sd=0.10)
pbinom(70,8,10)
qnorm(0.7,mean=8,sd=10)
pnorm(0.7,mean=8,sd=10)
1100+1.96*75
1100+1.64*75
75*2
1100+150
pbinom(4,size=5,prob=.5,lower.tail=FALSE)
ppois(10, lambda=5*3)
pnorm(1000,mean=.5,sd=1/(sqrt(12)))
pnorm(0.7,mean=80,sd=10)
pnorm(70,mean=80,sd=10)
choose(5,4)*(0.5^4)*((0.5)^1)
choose(5,4)*(0.5^4)*((0.5)^1)+choose(5,5)*(0.5^5)*((0.5)^0)
library("swirl")
swilr()
swirl()
myplot(2)
myplot(20)
myplot2()
myplot2(2)
qt(0.975,2)
myplot2(20)
sleep
range(g1)
range(g2)
difference=g1-g2
difference<-g1-g2
difference<-g2-g1
mean(difference)
s=sd(difference)
s<-sd(difference)
1.75c(-1,1)*0.975*(s/sqrt(10))
1.75+c(-1,1)*0.975*(s/sqrt(10))
mn+c(-1,1)*qt(0.975,9)*(s/sqrt(10))
mn+c(-1,1)*qt(0.975,9)*s/sqrt(10)
t.test$conf.int
t.test(difference)$conf.int
sp<-((8-1)*15.34)^2+((21-1)*18.23)^2
sp<-(7*15.34)^2+(20*18.23)^2
sp<-7*15.34^2+20*18.23^2
ns<-7+20
sp<-sqrt(sp/ns)
132.86-127.44+c(-1,1)*sp*sqrt(1/8+1/21)
132.86-127.44+c(-1,1)*qt(0.975,ns)*sp*sqrt(1/8+1/21)
sp<-sqrt()
sp<-sqrt(g1-g2)
sp<-sqrt((9*var(g1)+9*var(g2))/18)
132.86-127.44+c(-1,1)*qt(0.975,ns)*sp*sqrt(1/10+1/10)
md+c(-1,1)*qt(0.975,18)*sp*sqrt(1/5)
t.test(g2,g2,paired=FALSE)$conf.int
t.test(g2,g1,paired=FALSE)$conf.int
t.test(g2,g1,paired=FALSE,var.equal=TRUE)$conf.int
t.test(g2,g1,paired=FALSE,var.equal=TRUE)$conf
t.test(g2,g1,paired=FALSE)$conf
t.test(g2,g1,paired=TRUE)$conf
num<-8+21-2
num<-(15.34^2/8 + 18.23^2/21)^2
den<-
;
den<-15.34*8+18.23*21
den <- 15.34^4/8^2/7 + 18.23^4/21^2/20
mydf<-num/den
132.86-15.34+c(-1,1)*qt(0.975,mydf)*sp*sqrt(15.34^2/7+8^2/20)
132.86-15.34+c(-1,1)*qt(0.975,mydf)*sp*sqrt(15.34^2/8+8^2/21)
132.86-127.44+c(-1,1)*qt(0.975,mydf)*sp*sqrt(15.34^2/8+8^2/21)
132.86-127.44+c(-1,1)*qt(0.975,mydf)*sqrt(15.34^2/8+18.23^2/21)
10/sqrt(100)
swirl()
swirl()
1100+c(-1,1)*0.975*(30/sqrt(9))
t.test(-2)$conf.int
1100+c(-1,1)*qt(0.975,8)*(30/sqrt(9))
output: pdf_document
install.packages("knitr")
q()
library("knitr")
install.packages("shiny")
install.packages("devtools")
devtools::install_github('rstudio/rsconnect')
rsconnect::setAccountInfo(name='aadeshnpn', token='30F23F12DBF5FEFEBB970C1B1EA080FE', secret='JDPL0T6fN7UoN1SFqaMLYox7dW+72oDdqvZwruy6')
library(rsconnect)
install.packages("shiny")
install.packages("shiny")
diseaseModel<- function(){
#Load the custom made R dataset
load("/home/tensor/Documents/Tutorials/R/Ninth/data/disease.RData")
d1<-d1[-c(40),]
#set.seed(13)
#d1
d1$disease<-factor(d1$disease)
#Creating corpus
disease_corpus <- Corpus(VectorSource(d1$symptoms))
disease_corpus_clean <- disease_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
disease_dtm <- DocumentTermMatrix(disease_corpus_clean)
#dataset
train_index <- createDataPartition(d1$disease, p=1, list=FALSE)
#train_index
disease_raw<-d1[train_index,]
disease_corpus_clean_train <- disease_corpus_clean[train_index]
disease_dtm_train <- disease_dtm[train_index,]
#Getting frequency
disease_dict <- findFreqTerms(disease_dtm_train, lowfreq=1)
disease_train <- DocumentTermMatrix(disease_corpus_clean_train, list(dictionary=disease_dict))
#Model
ctrl <- trainControl(method="cv", 10)
set.seed(13)
disease_model <- train(disease_raw, disease_raw$disease, method="nb",
trControl=ctrl)
#return(disease_model)
save(disease_model,"dmodel.rds")
#predictedval<<-predict(disease_model,sympt,type="prob")
}
diseaseModel()
library(shiny)
library("tm")
library("SnowballC")
library("klaR")
library("MASS")
library("caret")
library("pander")
library("dplyr")
diseaseModel<- function(){
#Load the custom made R dataset
load("/home/tensor/Documents/Tutorials/R/Ninth/data/disease.RData")
d1<-d1[-c(40),]
#set.seed(13)
#d1
d1$disease<-factor(d1$disease)
#Creating corpus
disease_corpus <- Corpus(VectorSource(d1$symptoms))
disease_corpus_clean <- disease_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
disease_dtm <- DocumentTermMatrix(disease_corpus_clean)
#dataset
train_index <- createDataPartition(d1$disease, p=1, list=FALSE)
#train_index
disease_raw<-d1[train_index,]
disease_corpus_clean_train <- disease_corpus_clean[train_index]
disease_dtm_train <- disease_dtm[train_index,]
#Getting frequency
disease_dict <- findFreqTerms(disease_dtm_train, lowfreq=1)
disease_train <- DocumentTermMatrix(disease_corpus_clean_train, list(dictionary=disease_dict))
#Model
ctrl <- trainControl(method="cv", 10)
set.seed(13)
disease_model <- train(disease_raw, disease_raw$disease, method="nb",
trControl=ctrl)
#return(disease_model)
save(disease_model,"dmodel.rds")
#predictedval<<-predict(disease_model,sympt,type="prob")
}
diseaseModel()
library(shiny)
library("tm")
library("SnowballC")
library("klaR")
library("MASS")
library("caret")
library("pander")
library("dplyr")
diseaseModel<- function(){
#Load the custom made R dataset
load("/home/tensor/Documents/Tutorials/R/Ninth/data/disease.RData")
d1<-d1[-c(40),]
#set.seed(13)
#d1
d1$disease<-factor(d1$disease)
#Creating corpus
disease_corpus <- Corpus(VectorSource(d1$symptoms))
disease_corpus_clean <- disease_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
disease_dtm <- DocumentTermMatrix(disease_corpus_clean)
#dataset
train_index <- createDataPartition(d1$disease, p=1, list=FALSE)
#train_index
disease_raw<-d1[train_index,]
disease_corpus_clean_train <- disease_corpus_clean[train_index]
disease_dtm_train <- disease_dtm[train_index,]
#Getting frequency
disease_dict <- findFreqTerms(disease_dtm_train, lowfreq=1)
disease_train <- DocumentTermMatrix(disease_corpus_clean_train, list(dictionary=disease_dict))
#Model
ctrl <- trainControl(method="cv", 10)
set.seed(13)
disease_model <- train(disease_raw, disease_raw$disease, method="nb",
trControl=ctrl)
#return(disease_model)
save(disease_model,file="dmodel.rds")
#predictedval<<-predict(disease_model,sympt,type="prob")
}
diseaseModel()
getwd()
setwd("/home/tensor/Documents/Tutorials/R/Ninth/data")
library(shiny)
library("tm")
library("SnowballC")
library("klaR")
library("MASS")
library("caret")
library("pander")
library("dplyr")
diseaseModel<- function(){
#Load the custom made R dataset
load("/home/tensor/Documents/Tutorials/R/Ninth/data/disease.RData")
d1<-d1[-c(40),]
#set.seed(13)
#d1
d1$disease<-factor(d1$disease)
#Creating corpus
disease_corpus <- Corpus(VectorSource(d1$symptoms))
disease_corpus_clean <- disease_corpus %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removeWords, stopwords(kind="en")) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
disease_dtm <- DocumentTermMatrix(disease_corpus_clean)
#dataset
train_index <- createDataPartition(d1$disease, p=1, list=FALSE)
#train_index
disease_raw<-d1[train_index,]
disease_corpus_clean_train <- disease_corpus_clean[train_index]
disease_dtm_train <- disease_dtm[train_index,]
#Getting frequency
disease_dict <- findFreqTerms(disease_dtm_train, lowfreq=1)
disease_train <- DocumentTermMatrix(disease_corpus_clean_train, list(dictionary=disease_dict))
#Model
ctrl <- trainControl(method="cv", 10)
set.seed(13)
disease_model <- train(disease_raw, disease_raw$disease, method="nb",
trControl=ctrl)
#return(disease_model)
save(disease_model,file="dmodel.rds")
#predictedval<<-predict(disease_model,sympt,type="prob")
}
diseaseModel()
shiny::runApp('~/Documents/Tutorials/R/Ninth')
load("dmodel.rds")
dmodel
dmodel<-load("dmodel.rds")
dmodel
dmodel$disease_model
dmodel
diseaseModel()
dmodel<-load("dmodel.rds")
ls()
dmodel
disease_model
load("dmodel.rds")
disease_model
shiny::runApp('~/Documents/Tutorials/R/Ninth')
shiny::runApp('~/Documents/Tutorials/R/Ninth')
shiny::runApp('~/Documents/Tutorials/R/Ninth')
